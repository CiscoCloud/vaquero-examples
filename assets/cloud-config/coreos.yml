#cloud-config
{{ if index .host "rootvol"}}
{{ template "coreos_cloudconfig" . }}
{{ else }}
{{ template "cloud_writefile" . }}
{{ end }}

{{ define "cloud_writefile" }}
write_files:
  - path: "/tmp/setup.sh"
    permissions: "0755"
    owner: "root"
    content: |
      #!/bin/bash

      # find and format root device
      ROOTDEV=`lsblk -p | grep disk | awk '{print $1}' | head -1`
      sgdisk --zap-all $ROOTDEV
      parted -s $ROOTDEV mktable gpt
      parted -s $ROOTDEV mkpart root 0% 100%
      ROOTDEV=${ROOTDEV}1

      mkfs.ext4 -c -L ROOTVOL -F ${ROOTDEV}

      # need to reboot to pick up the new root volume
      sync
      fsck -y ${ROOTDEV}
      if [ $? -ne 0 ]; then
         echo "FSCK FAILED!  CHECK FILESYTEM MANUALLY!!" > /dev/console
      else
         # tell Foreman about the new rootvol and that the build is complete
         curl -L -H "Content-Type:application/json" -H "Accept:application/json,version=2" -k -u api:c0mcast\! -X POST -d "{\"parameter\":{\"name\":\"rootvol\",\"value\":\"${ROOTDEV}\"}}" https://10.22.81.100/api/hosts/`hostname`/parameters
         #curl -L -H "Content-Type:application/json" -H "Accept:application/json,version=2" -k -u api:c0mcast\! -X PUT -d "{\"build\":\"false\"}" https://10.22.81.100/api/hosts/`hostname`

         # tell Foreman about the new rootvol and that the build is complete
         curl "{{.agent.url}}/cloud-config?{{range $k, $v := .host.selectors}}{{$k}}={{$v}}&{{end}}rootvol=installed" -o cloud.cfg 

         coreos-install -d ${ROOTDEV} -i cloud.cfg

         sleep 5
         reboot
      fi
{{ end }}
{{ define "coreos_cloudconfig" }}
coreos:

  update:
    group: stable
    reboot-strategy: off

  etcd2:
  {{ if eq .host.role "kube-master" }}
    # local master enpoints
    name: {{ .host.mstr }}
    initial-advertise-peer-urls: http://{{.host.ip}}:2380
    advertise-client-urls: http://{{.host.ip}}:2379
    listen-peer-urls: http://0.0.0.0:2380

    # cross-origin resource sharing
    cors: "*"
    # TODO: check that role == "kube-node" here?
  {{ else }}
    proxy: on
  {{ end }}
    # cluster endpoints
    listen-client-urls: http://0.0.0.0:2379
    initial-cluster: {{ range $i, $node := .env.etcd_nodes }}{{ if $i }},{{end}}{{$node.name}}=http://{{$node.ip}}:2380{{end}}

  fleet:
  {{ if index .host "mstr" }}
    metadata: "role={{.host.role}},kube-master={{.host.mstr}},vlan={{.host.vlan}},vlan-{{.host.vlan}}=true{{if index .host "fleetadd"}}{{.host.fleetadd}}{{end}}"
  {{ else }}
    metadata: "role={{.host.role}},vlan={{.host.vlan}},vlan-{{.host.vlan}}=true{{if index .host "fleetadd"}}{{.host.fleetadd}}{{end}}"
  {{ end }}

  units:
  #TODO: This looks dirty :(
{{ if index .host "rootvol" | not}}
    - name: setup.service
      command: start
      content: |
        [Unit]
        Description=Setup VIPER CoreOS
        Author=VIPER

        [Service]
        ExecStart=/tmp/setup.sh
{{ end }}
    - name: etcd2.service
      command: start
    - name: fleet.service
      command: start
    - name: docker.service
      drop-ins:
        - name: 10-docker-opts.conf
          content: |
            [Service]
            # Setting max log size to 10M
            Environment="DOCKER_OPTS=--log-driver=json-file --log-opt max-size=10m"
    - name: early-docker.service
      drop-ins:
        - name: 10-docker-opts.conf
          content: |
            [Service]
            # Setting max log size to 10M
            Environment="DOCKER_OPTS=--log-driver=json-file --log-opt max-size=10m"

{{ if index .env "ssh_authorized_keys" }}
ssh_authorized_keys:
  {{ range $element := .env.ssh_authorized_keys }}
  - {{$element}}
  {{ end }}
{{ else }}
users:
  - name: core
    passwd: {{ .group.root_pass }}
{{ end }}
